### ì¡°ê¸ˆ ë” ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ìƒì„± ìœ„í•´ ì½”ë“œ ìˆ˜ì • ì¤‘. #####

from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel
import torch

# í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = PreTrainedTokenizerFast.from_pretrained(
    "skt/kogpt2-base-v2",
    bos_token='</s>',
    eos_token='</s>',
    unk_token='<unk>',
    pad_token='<pad>',
    mask_token='<mask>'
)

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')

# ì‚¬ìš©ìë¡œë¶€í„° ì…ë ¥ ë°›ê¸°
text = input("ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”: ")

# ì…ë ¥ ë¬¸ì¥ í† í¬ë‚˜ì´ì¦ˆ ë° í…ì„œ ë³€í™˜
input_ids = tokenizer.encode(text, return_tensors='pt')

# ë¬¸ì¥ ìƒì„±
gen_ids = model.generate(
    input_ids,
    max_length=128,
    repetition_penalty=2.0,
    pad_token_id=tokenizer.pad_token_id,
    eos_token_id=tokenizer.eos_token_id,
    bos_token_id=tokenizer.bos_token_id,
    use_cache=True
)

# ê²°ê³¼ ë””ì½”ë”©
generated = tokenizer.decode(gen_ids[0], skip_special_tokens=True)

# ì¶œë ¥
print("\nğŸ“¢ ìƒì„±ëœ ë¬¸ì¥:")
print(generated)
